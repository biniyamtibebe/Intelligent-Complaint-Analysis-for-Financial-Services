{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be27160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 128483 complaints after filtering and cleaning.\n",
      "Loaded 128483 complaints after filtering and cleaning.\n",
      "Product distribution after mapping:\n",
      "product_category\n",
      "Credit Cards        102727\n",
      "Savings Accounts     14845\n",
      "Personal Loans        9417\n",
      "Money Transfers       1494\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# notebooks/task2_chunking_embedding_indexing.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load the cleaned dataset\n",
    "# -----------------------------\n",
    "data_path = r\"c:\\Users\\hp\\Pictures\\financial intelligence\\Intelligent-Complaint-Analysis-for-Financial-Services\\rag-complaint-chatbot\\data\\processed\\filtered1_complaints.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} complaints after filtering and cleaning.\")\n",
    "print(f\"Loaded {len(df)} complaints after filtering and cleaning.\")\n",
    "\n",
    "# Ensure product column exists and map to consistent categories for CrediTrust\n",
    "# Adjust these mappings based on actual values in your filtered data\n",
    "product_mapping = {\n",
    "    'Credit card': 'Credit Cards',\n",
    "    'Credit card or prepaid card': 'Credit Cards',\n",
    "    'Consumer Loan': 'Personal Loans',\n",
    "    'Vehicle loan or lease': 'Personal Loans',  # sometimes included\n",
    "    'Bank account or service': 'Savings Accounts',\n",
    "    'Checking or savings account': 'Savings Accounts',\n",
    "    'Money transfer': 'Money Transfers',\n",
    "    'Money transfers': 'Money Transfers',\n",
    "    'Virtual currency': 'Money Transfers',  # optional inclusion\n",
    "}\n",
    "\n",
    "# Create a standardized product_category column\n",
    "df['product_category'] = df['Product'].map(product_mapping)\n",
    "\n",
    "# Drop rows where product_category is NaN (not in our target products)\n",
    "df = df.dropna(subset=['product_category']).reset_index(drop=True)\n",
    "\n",
    "print(\"Product distribution after mapping:\")\n",
    "print(df['product_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9e7d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified sample created: 12000 complaints\n",
      "Sample distribution:\n",
      "product_category\n",
      "Credit Cards        9594\n",
      "Savings Accounts    1386\n",
      "Personal Loans       880\n",
      "Money Transfers      140\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_8848\\4095402053.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stratified_sample = df.groupby('product_category', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# 2. Create Stratified Sample (10,000 - 15,000 complaints)\n",
    "# -----------------------------\n",
    "SAMPLE_SIZE = 12000  # Target size — adjust if needed\n",
    "\n",
    "# Stratified sampling proportional to product_category\n",
    "stratified_sample = df.groupby('product_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=SAMPLE_SIZE / len(df), random_state=42)\n",
    ")\n",
    "\n",
    "# If frac results in more/less than desired, cap or upsample\n",
    "if len(stratified_sample) > SAMPLE_SIZE:\n",
    "    stratified_sample = stratified_sample.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "elif len(stratified_sample) < SAMPLE_SIZE:\n",
    "    # Upsample minority classes if needed\n",
    "    stratified_sample = stratified_sample.sample(n=SAMPLE_SIZE, replace=True, random_state=42)\n",
    "\n",
    "stratified_sample = stratified_sample.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nStratified sample created: {len(stratified_sample)} complaints\")\n",
    "print(\"Sample distribution:\")\n",
    "print(stratified_sample['product_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704f9bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example chunks (first complaint): 2 chunks created\n"
     ]
    }
   ],
   "source": [
    "# 3. Text Chunking Strategy\n",
    "# -----------------------------\n",
    "# We use RecursiveCharacterTextSplitter — it tries to split on paragraphs, then sentences, etc.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,          # ~100-150 words per chunk (good for MiniLM context)\n",
    "    chunk_overlap=50,        # Small overlap helps maintain context across chunks\n",
    "    length_function=len,     # Measure length in characters\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Default recursive order\n",
    ")\n",
    "\n",
    "# Test on one example\n",
    "example_text = stratified_sample['cleaned_narrative'].iloc[0]\n",
    "chunks_example = text_splitter.split_text(example_text)\n",
    "print(f\"\\nExample chunks (first complaint): {len(chunks_example)} chunks created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions\n",
    "# all-MiniLM-L6-v2: 384-dim, fast, excellent performance on semantic similarity\n",
    "# Trained on 1B+ sentence pairs — ideal for complaint retrieval\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Chroma embedding function\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f636e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking all narratives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [00:14<00:00, 843.63it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 34645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Prepare Chunks with Metadata\n",
    "# -----------------------------\n",
    "chunks = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "documents = []  # raw text chunks\n",
    "\n",
    "print(\"\\nChunking all narratives...\")\n",
    "for idx, row in tqdm(stratified_sample.iterrows(), total=len(stratified_sample)):\n",
    "    narrative = row['cleaned_narrative']\n",
    "    complaint_id = str(row.get('Complaint ID', f\"complaint_{idx}\"))  # Use actual ID if available\n",
    "    \n",
    "    # Split into chunks\n",
    "    text_chunks = text_splitter.split_text(narrative)\n",
    "    \n",
    "    for chunk_idx, chunk_text in enumerate(text_chunks):\n",
    "        if len(chunk_text.strip()) < 20:  # Skip very tiny chunks\n",
    "            continue\n",
    "            \n",
    "        chunk_id = f\"{complaint_id}_chunk{chunk_idx}\"\n",
    "        \n",
    "        ids.append(chunk_id)\n",
    "        documents.append(chunk_text)\n",
    "        metadatas.append({\n",
    "            \"complaint_id\": complaint_id,\n",
    "            \"product_category\": row['product_category'],\n",
    "            \"product\": row['Product'],\n",
    "            \"date_received\": str(row.get('Date received', '')),\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"total_chunks\": len(text_chunks),\n",
    "            \"source_narrative_length\": len(narrative)\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a614f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding chunks to ChromaDB in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [23:49<00:00, 20.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store successfully created and persisted at: vector_store/chroma_sample_index\n",
      "Collection contains 34645 vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Create and Populate ChromaDB Vector Store\n",
    "# -----------------------------\n",
    "persist_directory = \"vector_store/chroma_sample_index\"\n",
    "\n",
    "# Initialize Chroma client with persistence\n",
    "client = chromadb.PersistentClient(path=persist_directory)\n",
    "\n",
    "# Create or get collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"complaint_chunks_sample\",\n",
    "    embedding_function=embedding_function,  # Chroma will embed on add if needed\n",
    "    metadata={\"hnsw:space\": \"cosine\"}      # Cosine similarity — best for text embeddings\n",
    ")\n",
    "\n",
    "# Add in batches to avoid memory issues\n",
    "batch_size = 500\n",
    "print(\"\\nAdding chunks to ChromaDB in batches...\")\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    batch_docs = documents[i:i+batch_size]\n",
    "    batch_ids = ids[i:i+batch_size]\n",
    "    batch_metas = metadatas[i:i+batch_size]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=batch_docs,\n",
    "        ids=batch_ids,\n",
    "        metadatas=batch_metas\n",
    "    )\n",
    "\n",
    "print(f\"Vector store successfully created and persisted at: {persist_directory}\")\n",
    "print(f\"Collection contains {collection.count()} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcab60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Retrieval Results ---\n",
      "Distance: 0.1613\n",
      "Product: Credit Cards\n",
      "Snippet: i experienced 2 imposition of late fees on late fees following the delayed payment i was charged a late fee however to my dismay i discovered that i was subsequently charged a late fee on the previous...\n",
      "\n",
      "Distance: 0.1798\n",
      "Product: Credit Cards\n",
      "Snippet: late fees was charged to which i had to pay to protect my credit...\n",
      "\n",
      "Distance: 0.1805\n",
      "Product: Personal Loans\n",
      "Snippet: charging late fees with no reason payments made on time claiming excessive and unfounded late fees...\n",
      "\n",
      "Distance: 0.1962\n",
      "Product: Credit Cards\n",
      "Snippet: assessed a late payment fee on my card made payment on time...\n",
      "\n",
      "Distance: 0.2013\n",
      "Product: Credit Cards\n",
      "Snippet: i am currently being overcharged for late fees on a credit card that i am currently not using i was not told anything about late fees or the excessive amount every month the late fee price is changed ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Quick Sanity Check: Perform a Test Query\n",
    "# -----------------------------\n",
    "test_results = collection.query(\n",
    "    query_texts=[\"late fees charged unfairly on credit card\"],\n",
    "    n_results=5,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Test Retrieval Results ---\")\n",
    "for doc, meta, dist in zip(test_results['documents'][0], \n",
    "                          test_results['metadatas'][0], \n",
    "                          test_results['distances'][0]):\n",
    "    print(f\"Distance: {dist:.4f}\")\n",
    "    print(f\"Product: {meta['product_category']}\")\n",
    "    print(f\"Snippet: {doc[:200]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".week7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
